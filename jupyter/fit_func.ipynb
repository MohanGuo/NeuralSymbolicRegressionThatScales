{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example for performing symbolic regression for a set of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nesymres.architectures.model import Model\n",
    "from nesymres.utils import load_metadata_hdf5\n",
    "from nesymres.dclasses import FitParams, NNEquation, BFGSParams\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import torch\n",
    "from sympy import lambdify\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load equation configuration and architecture configuration\n",
    "import omegaconf\n",
    "with open('100M/eq_setting.json', 'r') as json_file:\n",
    "  eq_setting = json.load(json_file)\n",
    "\n",
    "cfg = omegaconf.OmegaConf.load(\"100M/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up BFGS load rom the hydra config yaml\n",
    "bfgs = BFGSParams(\n",
    "        activated= cfg.inference.bfgs.activated,\n",
    "        n_restarts=cfg.inference.bfgs.n_restarts,\n",
    "        add_coefficients_if_not_existing=cfg.inference.bfgs.add_coefficients_if_not_existing,\n",
    "        normalization_o=cfg.inference.bfgs.normalization_o,\n",
    "        idx_remove=cfg.inference.bfgs.idx_remove,\n",
    "        normalization_type=cfg.inference.bfgs.normalization_type,\n",
    "        stop_time=cfg.inference.bfgs.stop_time,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.inference.beam_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_fit = FitParams(word2id=eq_setting[\"word2id\"], \n",
    "                            id2word={int(k): v for k,v in eq_setting[\"id2word\"].items()}, \n",
    "                            una_ops=eq_setting[\"una_ops\"], \n",
    "                            bin_ops=eq_setting[\"bin_ops\"], \n",
    "                            total_variables=list(eq_setting[\"total_variables\"]),  \n",
    "                            total_coefficients=list(eq_setting[\"total_coefficients\"]),\n",
    "                            rewrite_functions=list(eq_setting[\"rewrite_functions\"]),\n",
    "                            bfgs=bfgs,\n",
    "                            # beam_size=cfg.inference.beam_size #This parameter is a tradeoff between accuracy and fitting time\n",
    "                            beam_size=2\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"100M/100M.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load architecture, set into eval mode, and pass the config parameters\n",
    "model = Model.load_from_checkpoint(weights_path, cfg=cfg.architecture)\n",
    "model.eval()\n",
    "if torch.cuda.is_available(): \n",
    "  model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitfunc = partial(model.fitfunc,cfg_params=params_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create points from an equation\n",
    "number_of_points = 500\n",
    "n_variables = 1\n",
    "\n",
    "#To get best results make sure that your support inside the max and mix support\n",
    "max_supp = cfg.dataset_train.fun_support[\"max\"] \n",
    "min_supp = cfg.dataset_train.fun_support[\"min\"]\n",
    "X = torch.rand(number_of_points,len(list(eq_setting[\"total_variables\"])))*(max_supp-min_supp)+min_supp\n",
    "X[:,n_variables:] = 0\n",
    "target_eq = \"cos(sin(cos(x_1**2)*sin(x_1)))\" #Use x_1,x_2 and x_3 as independent variables\n",
    "X_dict = {x:X[:,idx].cpu() for idx, x in enumerate(eq_setting[\"total_variables\"])} \n",
    "y = lambdify(\",\".join(eq_setting[\"total_variables\"]), target_eq)(**X_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = fitfunc(X,y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_eq = \"cos(x_1**6)*sin(x_1)\" #Use x_1,x_2 and x_3 as independent variables\n",
    "target_eq = \"x_1**4\"\n",
    "X_dict = {x:X[:,idx].cpu() for idx, x in enumerate(eq_setting[\"total_variables\"])} \n",
    "y = lambdify(\",\".join(eq_setting[\"total_variables\"]), target_eq)(**X_dict)\n",
    "output = fitfunc(X,y) \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create points from an equation\n",
    "number_of_points = 500\n",
    "n_variables = 1\n",
    "\n",
    "#To get best results make sure that your support inside the max and mix support\n",
    "max_supp = cfg.dataset_train.fun_support[\"max\"] \n",
    "min_supp = cfg.dataset_train.fun_support[\"min\"]\n",
    "X = torch.rand(number_of_points,len(list(eq_setting[\"total_variables\"])))*(max_supp-min_supp)+min_supp\n",
    "X[:,n_variables:] = 0\n",
    "target_eq = \"cos(sin(cos(x_1**2)*sin(x_1)))\" #Use x_1,x_2 and x_3 as independent variables\n",
    "X_dict = {x:X[:,idx].cpu() for idx, x in enumerate(eq_setting[\"total_variables\"])} \n",
    "y = lambdify(\",\".join(eq_setting[\"total_variables\"]), target_eq)(**X_dict)\n",
    "\n",
    "output = fitfunc(X,y) \n",
    "output_all = output[\"all_bfgs_preds\"]\n",
    "loss_all = output[\"all_bfgs_loss\"]\n",
    "output_best = output[\"best_bfgs_preds\"]\n",
    "loss_best = output[\"best_bfgs_loss\"]\n",
    "print(f\"Best prediction: {output_best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP after BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms, gp\n",
    "import sympy as sp\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "\n",
    "# Ensure this block runs after obtaining `output_all` from the model\n",
    "# Create points from an equation\n",
    "number_of_points = 50\n",
    "n_variables = 1\n",
    "\n",
    "# To get best results make sure that your support inside the max and mix support\n",
    "max_supp = cfg.dataset_train.fun_support[\"max\"]\n",
    "min_supp = cfg.dataset_train.fun_support[\"min\"]\n",
    "X = torch.rand(number_of_points, len(list(eq_setting[\"total_variables\"]))) * (max_supp - min_supp) + min_supp\n",
    "X[:, n_variables:] = 0\n",
    "target_eq = \"cos(sin(cos(x_1**2) * sin(x_1)))\"  # Use x_1, x_2 and x_3 as independent variables\n",
    "X_dict = {x: X[:, idx].cpu() for idx, x in enumerate(eq_setting[\"total_variables\"])}\n",
    "y = lambdify(\",\".join(eq_setting[\"total_variables\"]), target_eq)(**X_dict)\n",
    "\n",
    "output = fitfunc(X, y)\n",
    "output_all = output[\"all_bfgs_preds\"]\n",
    "loss_all = output[\"all_bfgs_loss\"]\n",
    "output_best = output[\"best_bfgs_preds\"]\n",
    "loss_best = output[\"best_bfgs_loss\"]\n",
    "print(f\"Best prediction: {output_best}\")\n",
    "\n",
    "print(output_all)\n",
    "equations_str = output_all\n",
    "\n",
    "equations = []\n",
    "equation_strs = []  # To keep track of the original equation strings\n",
    "for eq_str in equations_str:\n",
    "    expr = sp.sympify(eq_str)\n",
    "    variables = sorted(expr.free_symbols, key=lambda x: x.name)\n",
    "    func = lambdify(variables, expr, modules=['numpy'])\n",
    "    equations.append(func)\n",
    "    equation_strs.append(eq_str)  # Store the original equation string\n",
    "\n",
    "def evaluateEquation(individual):\n",
    "    equation = individual[0]\n",
    "    y_pred = np.array([equation(x) for x in X[:, 0]])\n",
    "    y_true = np.array(y)\n",
    "    error = np.sum((y_true - y_pred) ** 2)\n",
    "    return (error,)\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"expr\", random.choice, equations)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.expr, 1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluateEquation)\n",
    "toolbox.register(\"mate\", gp.cxOnePoint)\n",
    "toolbox.register(\"mutate\", gp.mutNodeReplacement, pset=None) # Correct mutation function\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Optimized genetic programming parameters\n",
    "population_size = 200\n",
    "num_generations = 100\n",
    "crossover_prob = 0.7\n",
    "mutation_prob = 0.3\n",
    "\n",
    "population = toolbox.population(n=population_size)\n",
    "result = algorithms.eaSimple(population, toolbox, cxpb=crossover_prob, mutpb=mutation_prob, ngen=num_generations, verbose=True)\n",
    "\n",
    "best_ind = tools.selBest(population, 1)[0]\n",
    "best_index = equations.index(best_ind[0]) # Get the index of the best equation\n",
    "best_equation_str = equation_strs[best_index] # Retrieve the original equation string\n",
    "\n",
    "print('Best Equation:', best_equation_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_fit = FitParams(word2id=eq_setting[\"word2id\"], \n",
    "                            id2word={int(k): v for k,v in eq_setting[\"id2word\"].items()}, \n",
    "                            una_ops=eq_setting[\"una_ops\"], \n",
    "                            bin_ops=eq_setting[\"bin_ops\"], \n",
    "                            total_variables=list(eq_setting[\"total_variables\"]),  \n",
    "                            total_coefficients=list(eq_setting[\"total_coefficients\"]),\n",
    "                            rewrite_functions=list(eq_setting[\"rewrite_functions\"]),\n",
    "                            bfgs=bfgs,\n",
    "                            # beam_size=cfg.inference.beam_size #This parameter is a tradeoff between accuracy and fitting time\n",
    "                            beam_size=6\n",
    "                            )\n",
    "## Load architecture, set into eval mode, and pass the config parameters\n",
    "model = Model.load_from_checkpoint(weights_path, cfg=cfg.architecture)\n",
    "model.eval()\n",
    "if torch.cuda.is_available(): \n",
    "  model.cuda()\n",
    "fitfunc = partial(model.fitfunc,cfg_params=params_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms, gp\n",
    "import sympy as sp\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "\n",
    "# Ensure this block runs after obtaining `output_all` from the model\n",
    "# Create points from an equation\n",
    "number_of_points = 50\n",
    "n_variables = 1\n",
    "\n",
    "# To get best results make sure that your support inside the max and mix support\n",
    "max_supp = cfg.dataset_train.fun_support[\"max\"]\n",
    "min_supp = cfg.dataset_train.fun_support[\"min\"]\n",
    "X = torch.rand(number_of_points, len(list(eq_setting[\"total_variables\"]))) * (max_supp - min_supp) + min_supp\n",
    "X[:, n_variables:] = 0\n",
    "target_eq = \"cos(sin(cos(x_1**2) * sin(x_1)))\"  # Use x_1, x_2 and x_3 as independent variables\n",
    "X_dict = {x: X[:, idx].cpu() for idx, x in enumerate(eq_setting[\"total_variables\"])}\n",
    "y = lambdify(\",\".join(eq_setting[\"total_variables\"]), target_eq)(**X_dict)\n",
    "\n",
    "output = fitfunc(X, y)\n",
    "output_all = output[\"all_bfgs_preds\"]\n",
    "loss_all = output[\"all_bfgs_loss\"]\n",
    "output_best = output[\"best_bfgs_preds\"]\n",
    "loss_best = output[\"best_bfgs_loss\"]\n",
    "print(f\"Best prediction: {output_best}\")\n",
    "\n",
    "print(output_all)\n",
    "equations_str = output_all\n",
    "\n",
    "equations = []\n",
    "equation_strs = []  # To keep track of the original equation strings\n",
    "for eq_str in equations_str:\n",
    "    expr = sp.sympify(eq_str)\n",
    "    variables = sorted(expr.free_symbols, key=lambda x: x.name)\n",
    "    func = lambdify(variables, expr, modules=['numpy'])\n",
    "    equations.append(func)\n",
    "    equation_strs.append(eq_str)  # Store the original equation string\n",
    "\n",
    "def evaluateEquation(individual):\n",
    "    equation = individual[0]\n",
    "    y_pred = np.array([equation(x) for x in X[:, 0]])\n",
    "    y_true = np.array(y)\n",
    "    error = np.sum((y_true - y_pred) ** 2)\n",
    "    return (error,)\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"expr\", random.choice, equations)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.expr, 1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluateEquation)\n",
    "toolbox.register(\"mate\", gp.cxOnePoint)\n",
    "toolbox.register(\"mutate\", gp.mutNodeReplacement, pset=None) # Correct mutation function\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Optimized genetic programming parameters\n",
    "population_size = 200\n",
    "num_generations = 100\n",
    "crossover_prob = 0.7\n",
    "mutation_prob = 0.3\n",
    "\n",
    "population = toolbox.population(n=population_size)\n",
    "result = algorithms.eaSimple(population, toolbox, cxpb=crossover_prob, mutpb=mutation_prob, ngen=num_generations, verbose=True)\n",
    "\n",
    "best_ind = tools.selBest(population, 1)[0]\n",
    "best_index = equations.index(best_ind[0]) # Get the index of the best equation\n",
    "best_equation_str = equation_strs[best_index] # Retrieve the original equation string\n",
    "\n",
    "print('Best Equation:', best_equation_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import nesymres.architectures.model\n",
    "importlib.reload(nesymres.architectures.model)\n",
    "from nesymres.architectures.model import Model\n",
    "\n",
    "\n",
    "params_fit = FitParams(word2id=eq_setting[\"word2id\"], \n",
    "                            id2word={int(k): v for k,v in eq_setting[\"id2word\"].items()}, \n",
    "                            una_ops=eq_setting[\"una_ops\"], \n",
    "                            bin_ops=eq_setting[\"bin_ops\"], \n",
    "                            total_variables=list(eq_setting[\"total_variables\"]),  \n",
    "                            total_coefficients=list(eq_setting[\"total_coefficients\"]),\n",
    "                            rewrite_functions=list(eq_setting[\"rewrite_functions\"]),\n",
    "                            bfgs=bfgs,\n",
    "                            # beam_size=cfg.inference.beam_size #This parameter is a tradeoff between accuracy and fitting time\n",
    "                            beam_size=6\n",
    "                            )\n",
    "## Load architecture, set into eval mode, and pass the config parameters\n",
    "model = Model.load_from_checkpoint(weights_path, cfg=cfg.architecture)\n",
    "model.eval()\n",
    "if torch.cuda.is_available(): \n",
    "  model.cuda()\n",
    "fitfunc = partial(model.fitfunc,cfg_params=params_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create points from an equation\n",
    "number_of_points = 500\n",
    "n_variables = 1\n",
    "\n",
    "#To get best results make sure that your support inside the max and mix support\n",
    "max_supp = cfg.dataset_train.fun_support[\"max\"] \n",
    "min_supp = cfg.dataset_train.fun_support[\"min\"]\n",
    "X = torch.rand(number_of_points,len(list(eq_setting[\"total_variables\"])))*(max_supp-min_supp)+min_supp\n",
    "X[:,n_variables:] = 0\n",
    "target_eq = \"cos(sin(cos(x_1**2)*sin(x_1)))\" #Use x_1,x_2 and x_3 as independent variables\n",
    "X_dict = {x:X[:,idx].cpu() for idx, x in enumerate(eq_setting[\"total_variables\"])} \n",
    "y = lambdify(\",\".join(eq_setting[\"total_variables\"]), target_eq)(**X_dict)\n",
    "\n",
    "output = fitfunc(X,y) \n",
    "# output_all = output[\"all_bfgs_preds\"]\n",
    "# loss_all = output[\"all_bfgs_loss\"]\n",
    "# output_best = output[\"best_bfgs_preds\"]\n",
    "# loss_best = output[\"best_bfgs_loss\"]\n",
    "# print(f\"Best prediction: {output_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params_fit.id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nesymres.architectures.data import DataModule\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import hydra\n",
    "from pathlib import Path\n",
    "\n",
    "train_path = '/workspaces/NeuralSymbolicRegressionThatScales/data/datasets/10000/'\n",
    "val_path = '/workspaces/NeuralSymbolicRegressionThatScales/data/raw_datasets/150'\n",
    "\n",
    "seed_everything(9)\n",
    "# train_path = Path(hydra.utils.to_absolute_path(train_path))\n",
    "# val_path = Path(hydra.utils.to_absolute_path(val_path))\n",
    "data = DataModule(\n",
    "    train_path,\n",
    "    val_path,\n",
    "    None,\n",
    "    cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "class MLPModule(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.001):\n",
    "        super(MLPModule, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = F.mse_loss(outputs, targets)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = F.mse_loss(outputs, targets)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nesymres.architectures.data import DataModule\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import hydra\n",
    "from pathlib import Path\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    train_path = '/workspaces/NeuralSymbolicRegressionThatScales/data/datasets/10000/'\n",
    "    val_path = '/workspaces/NeuralSymbolicRegressionThatScales/data/raw_datasets/150'\n",
    "\n",
    "    seed_everything(9)\n",
    "    # train_path = Path(hydra.utils.to_absolute_path(train_path))\n",
    "    # val_path = Path(hydra.utils.to_absolute_path(val_path))\n",
    "    data_module = DataModule(\n",
    "        train_path,\n",
    "        val_path,\n",
    "        None,\n",
    "        cfg\n",
    "    )\n",
    "\n",
    "    input_size = 100\n",
    "    hidden_size = 50\n",
    "    output_size = 3\n",
    "    model = MLPModule(input_size, hidden_size, output_size)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        filename='best-checkpoint',\n",
    "        save_top_k=1,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    # wandb_logger = WandbLogger(project=\"my_project\")\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        # gpus=1,\n",
    "        # logger=wandb_logger,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设你有一个 datamodule 和 dataloader 已经设置好\n",
    "data.setup('fit')  # 如果尚未自动调用\n",
    "train_loader = data.train_dataloader()\n",
    "\n",
    "first_batch = next(iter(train_loader))\n",
    "print(type(first_batch))  # 查看数据类型应为 tuple\n",
    "# print(first_batch.shape)\n",
    "inputs, targets, whatsthis = first_batch  # 使用下划线忽略额外的信息\n",
    "print(\"Inputs shape:\", inputs.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "print(\"Targets shape:\", whatsthis.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"whatsthis shape:\", whatsthis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sympy as sp\n",
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "path_to_skeletons = '/workspaces/NeuralSymbolicRegressionThatScales/test_set/test_nc.csv'\n",
    "path_to_constants = '/workspaces/NeuralSymbolicRegressionThatScales/test_set/test_wc.csv'\n",
    "\n",
    "skeletons_df = pd.read_csv(path_to_skeletons)\n",
    "constants_df = pd.read_csv(path_to_constants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_support(support_str):\n",
    "    return json.loads(support_str.replace(\"'\", \"\\\"\"))\n",
    "\n",
    "skeletons_df['support'] = skeletons_df['support'].apply(parse_support)\n",
    "constants_df['support'] = constants_df['support'].apply(parse_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(constants_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_sample_points(support_dict, num_points):\n",
    "    samples = {var: np.random.uniform(bounds['min'], bounds['max'], num_points)\n",
    "               for var, bounds in support_dict.items()}\n",
    "    return pd.DataFrame(samples)\n",
    "\n",
    "\n",
    "# skeleton_samples = skeletons_df['support'].apply(lambda x: extract_sample_points(x, 500))\n",
    "# constant_samples = constants_df['support'].apply(lambda x: extract_sample_points(x, 500))\n",
    "def compute_y(df, num_points):\n",
    "    full_samples = []\n",
    "    for i, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Extracting samples\"):\n",
    "        # obtain equation\n",
    "        equation = parse_expr(row['eq'], transformations='all')\n",
    "        valid_samples_count = 0\n",
    "        sample_points_df = pd.DataFrame()\n",
    "        while valid_samples_count < num_points:\n",
    "            # obtain samples x\n",
    "            new_sample_points_df = extract_sample_points(row['support'], num_points - valid_samples_count)\n",
    "            # compute y, ignore complex y\n",
    "            new_sample_points_df['y'] = new_sample_points_df.apply(\n",
    "                lambda vals: equation.evalf(subs=vals.to_dict()), axis=1)\n",
    "            # print(f\"Equation: {equation}\")\n",
    "            # print(f\"Sample x: {new_sample_points_df}\")\n",
    "            # print(f\"Output of Sample: {new_sample_points_df['y']}\")\n",
    "            # remove complex\n",
    "            new_sample_points_df = new_sample_points_df[new_sample_points_df['y'].apply(lambda x: x.is_real)]\n",
    "            new_sample_points_df = new_sample_points_df.dropna(subset=['y'])\n",
    "            # valid samples\n",
    "            sample_points_df = pd.concat([sample_points_df, new_sample_points_df])\n",
    "            \n",
    "            valid_samples_count = sample_points_df.shape[0]\n",
    "        # obtain num_points samples\n",
    "        sample_points_df = sample_points_df.head(num_points)\n",
    "        full_samples.append(sample_points_df)\n",
    "    return full_samples\n",
    "\n",
    "# skeleton_y = compute_y(skeletons_df, 10)\n",
    "# constant_y = compute_y(constants_df, 10)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting samples: 100%|██████████| 100/100 [00:09<00:00, 10.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# print(constants_df['eq'])\n",
    "constant_y = compute_y(constants_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        x_2       x_1       x_3                  y\n",
      "0 -2.038740  5.543135  7.350590  -49.4450233301101\n",
      "1 -9.021707 -2.841883 -6.809667  -49.2126276444300\n",
      "2 -6.221746  0.189251  1.573105  -2.28853097374882\n",
      "3  2.832158 -2.453522  2.194735  -44.5415445659957\n",
      "4  4.297825 -7.403096 -7.731069   501.367948241069\n",
      "5 -6.972310  3.542027  9.089860  -79.0920486914688\n",
      "6 -9.475093 -8.817643 -8.215728  -76.3151995122842\n",
      "7 -6.890810 -5.632550  2.992358  -14.5898025183408\n",
      "8 -4.818363  0.763671 -3.135211  -9.04054641968066\n",
      "9  5.964885 -7.801742 -2.611364   1002.52634593528\n"
     ]
    }
   ],
   "source": [
    "print(constant_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skeleton embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import nesymres.architectures.model\n",
    "importlib.reload(nesymres.architectures.model)\n",
    "from nesymres.architectures.model import Model\n",
    "\n",
    "\n",
    "params_fit = FitParams(word2id=eq_setting[\"word2id\"], \n",
    "                            id2word={int(k): v for k,v in eq_setting[\"id2word\"].items()}, \n",
    "                            una_ops=eq_setting[\"una_ops\"], \n",
    "                            bin_ops=eq_setting[\"bin_ops\"], \n",
    "                            total_variables=list(eq_setting[\"total_variables\"]),  \n",
    "                            total_coefficients=list(eq_setting[\"total_coefficients\"]),\n",
    "                            rewrite_functions=list(eq_setting[\"rewrite_functions\"]),\n",
    "                            bfgs=bfgs,\n",
    "                            # beam_size=cfg.inference.beam_size #This parameter is a tradeoff between accuracy and fitting time\n",
    "                            beam_size=2\n",
    "                            )\n",
    "## Load architecture, set into eval mode, and pass the config parameters\n",
    "model = Model.load_from_checkpoint(weights_path, cfg=cfg.architecture)\n",
    "model.eval()\n",
    "if torch.cuda.is_available(): \n",
    "  model.cuda()\n",
    "bs_results = partial(model.bs_results,cfg_params=params_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_model_outputs(sample_dataframes):\n",
    "    outputs = []\n",
    "    for df in tqdm(sample_dataframes, desc=\"Getting skeletons\"):\n",
    "    # for df in sample_dataframes:\n",
    "        X = df.drop(columns=['y']).values  # x is a vector except y\n",
    "        y = df['y'].values\n",
    "        y = np.array(y, dtype=np.float32)\n",
    "        # print(y)\n",
    "        output = bs_results(X, y)\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "\n",
    "skeleton_outputs = generate_model_outputs(skeleton_y)\n",
    "constant_outputs = generate_model_outputs(constant_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Tensors:\n",
      "[[ 1 18  6 ...  0  0  0]\n",
      " [ 1  9 18 ...  0  0  0]\n",
      " [ 1 20 18 ...  0  0  0]\n",
      " ...\n",
      " [ 1  9 19 ...  0  0  0]\n",
      " [ 1  9  5 ...  0  0  0]\n",
      " [ 1 18  6 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "def pad_sequences(sequences, maxlen, dtype='int', padding='post', truncating='post', value=0):\n",
    "    \"\"\"Pad each sequence to the same length: the length of the longest sequence.\"\"\"\n",
    "    padded_sequences = np.full((len(sequences), maxlen), fill_value=value, dtype=dtype)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) > maxlen:\n",
    "            if truncating == 'pre':\n",
    "                truncated = seq[-maxlen:]\n",
    "            else:\n",
    "                truncated = seq[:maxlen]\n",
    "        else:\n",
    "            truncated = seq\n",
    "        if padding == 'post':\n",
    "            padded_sequences[i, :len(truncated)] = truncated\n",
    "        else:\n",
    "            padded_sequences[i, -len(truncated):] = truncated\n",
    "    return padded_sequences\n",
    "\n",
    "# Extract tensors from the skeleton_outputs, ignoring scores\n",
    "tensors = [output[0][1].numpy() for output in skeleton_outputs]  # Assuming these are PyTorch tensors\n",
    "\n",
    "# Pad the extracted tensors to a maximum length of 20\n",
    "max_length = 20\n",
    "padded_tensors = pad_sequences(tensors, maxlen=max_length, dtype='int')\n",
    "\n",
    "print(\"Padded Tensors:\")\n",
    "print(padded_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        x_2       x_1       x_3                  y\n",
      "0 -2.038740  5.543135  7.350590  -49.4450233301101\n",
      "1 -9.021707 -2.841883 -6.809667  -49.2126276444300\n",
      "2 -6.221746  0.189251  1.573105  -2.28853097374882\n",
      "3  2.832158 -2.453522  2.194735  -44.5415445659957\n",
      "4  4.297825 -7.403096 -7.731069   501.367948241069\n",
      "5 -6.972310  3.542027  9.089860  -79.0920486914688\n",
      "6 -9.475093 -8.817643 -8.215728  -76.3151995122842\n",
      "7 -6.890810 -5.632550  2.992358  -14.5898025183408\n",
      "8 -4.818363  0.763671 -3.135211  -9.04054641968066\n",
      "9  5.964885 -7.801742 -2.611364   1002.52634593528\n"
     ]
    }
   ],
   "source": [
    "print(constant_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15192/1354399185.py:6: RuntimeWarning: overflow encountered in cast\n",
      "  y_features = np.array(y_features, dtype=np.float32)\n",
      "Combining embeddings and data points:   0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_15192/1354399185.py:22: RuntimeWarning: overflow encountered in cast\n",
      "  sample_y = np.array(sample_y, dtype=np.float32)\n",
      "Combining embeddings and data points: 100%|██████████| 100/100 [00:00<00:00, 1605.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined features shape: (100, 260)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_samples_df = pd.concat(constant_y, ignore_index=True)\n",
    "# print(all_samples_df)\n",
    "# obtain features\n",
    "X_features = all_samples_df.drop(columns=['y']).values  \n",
    "y_features = all_samples_df['y'].values\n",
    "y_features = np.array(y_features, dtype=np.float32)\n",
    "\n",
    "# padded_tensors is the output of transformers\n",
    "padded_tensors = np.array(padded_tensors)\n",
    "\n",
    "if padded_tensors.shape[0] != len(constant_y):\n",
    "    raise ValueError(\"Number of samples in constant_y does not match number of samples in padded_tensors\")\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "# iterate all samples\n",
    "for i, tensor in tqdm(enumerate(padded_tensors), total=len(padded_tensors), desc=\"Combining embeddings and data points\"):\n",
    "    sample_df = constant_y[i]\n",
    "    sample_features = sample_df.drop(columns=['y']).values\n",
    "    sample_y = sample_df['y'].values\n",
    "    sample_y = np.array(sample_y, dtype=np.float32)\n",
    "    combined_features = tensor\n",
    "    # combine embedding, sample_features and sample_y\n",
    "    for j in range(10):\n",
    "        max_length_feature = 23\n",
    "        x_feature = sample_features[j]\n",
    "        if x_feature.shape[0] < max_length_feature:\n",
    "            padding_length = max_length_feature - x_feature.shape[0]\n",
    "            x_feature = np.pad(x_feature, (0, padding_length), 'constant')\n",
    "        combined_features = np.hstack([combined_features, x_feature])\n",
    "        # print(f\"Feature shape in x :{combined_features.shape}\"\n",
    "        \n",
    "        # if combined_features.shape[0] < max_length_feature:\n",
    "            # padding_length = max_length_feature - combined_features.shape[0]\n",
    "            # combined_features = np.pad(combined_features, (0, padding_length), 'constant')\n",
    "        combined_features = np.hstack([combined_features,sample_y[j]])\n",
    "    \n",
    "        # print(f\"Shape: {tensor.shape}, {sample_features.shape}, {sample_y.shape}\")\n",
    "        # print(combined_features.shape)\n",
    "        # y_data.append(sample_y[j])\n",
    "    X_data.append(combined_features)\n",
    "    # print(f\"X shape: {combined_features.shape}\")\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "# y_data = np.array(y_data)\n",
    "\n",
    "print(\"Combined features shape:\", X_data.shape)\n",
    "# print(\"Target shape:\", y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skeleton_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting samples: 100%|██████████| 100/100 [00:09<00:00, 10.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Tensors shape: (100, 20)\n",
      "Constant Y shape:         x_2       x_1       x_3                  y\n",
      "0 -4.787315 -7.000346  7.448065  -62.5360934657640\n",
      "1 -2.362670  1.560538 -1.911053  -1.91162257703467\n",
      "2 -2.583387 -0.106637 -3.326650  -10.9220133750830\n",
      "3  6.529477  3.113005 -2.213097   1514.27401881362\n",
      "4 -5.464418  4.863633 -0.713338   4.35780322806024\n",
      "5 -7.047403  2.984540  0.416318   2.81085770988115\n",
      "6  5.239405  7.332776 -7.780136   1413.80814146908\n",
      "7 -7.605424  4.063913  7.956521  -59.2462666225795\n",
      "8 -1.686573 -7.093847  5.059850  -33.6327789085535\n",
      "9  0.690136  0.620391 -0.039348  0.697302158187894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "constant_y = compute_y(constants_df, 10)\n",
    "print(\"Padded Tensors shape:\", padded_tensors.shape)\n",
    "print(\"Constant Y shape:\", constant_y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant Labels Shape: (100,)\n",
      "Padded Constant Labels Shape: (100, 5)\n"
     ]
    }
   ],
   "source": [
    "def extract_constants(equation):\n",
    "    constants = []\n",
    "    for atom in equation.atoms(sp.Number):\n",
    "        if atom != sp.S.One:\n",
    "            constants.append(float(atom))\n",
    "    return constants\n",
    "\n",
    "# extract constants of equation.\n",
    "def get_constants_labels(df):\n",
    "    labels = []\n",
    "    for i, row in df.iterrows():\n",
    "        eq_str = row['eq']\n",
    "        equation = parse_expr(eq_str, transformations='all')\n",
    "        constants = extract_constants(equation)\n",
    "        labels.append(constants)\n",
    "    return labels\n",
    "\n",
    "constant_labels = get_constants_labels(constants_df)\n",
    "constant_labels_array = np.array(constant_labels, dtype=object)\n",
    "\n",
    "print(\"Constant Labels Shape:\", constant_labels_array.shape)\n",
    "max_length = max(len(label) for label in constant_labels)\n",
    "padded_labels = np.array([np.pad(label, (0, max_length - len(label)), 'constant') for label in constant_labels])\n",
    "\n",
    "# print out the output\n",
    "print(\"Padded Constant Labels Shape:\", padded_labels.shape)\n",
    "# print(\"Padded Constant Labels:\", padded_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# a simple regression model\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(X, y, batch_size=32):\n",
    "    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32))\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=1e-3):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            # print(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 260)\n",
      "(100, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_data.shape)\n",
    "print(padded_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 2/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 3/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 4/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 5/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 6/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 7/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 8/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 9/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 10/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 11/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 12/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 13/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 14/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 15/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 16/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 17/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 18/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 19/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 20/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 21/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 22/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 23/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 24/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 25/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 26/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 27/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 28/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 29/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 30/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 31/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 32/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 33/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 34/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 35/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 36/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 37/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 38/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 39/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 40/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 41/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 42/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 43/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 44/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 45/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 46/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 47/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 48/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 49/50, Training Loss: nan, Validation Loss: nan\n",
      "Epoch 50/50, Training Loss: nan, Validation Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# data prepare\n",
    "train_loader, val_loader = prepare_dataloader(X_data, padded_labels, batch_size=32)\n",
    "\n",
    "# model initialization\n",
    "input_dim = X_data.shape[1]\n",
    "output_dim = padded_labels.shape[1]\n",
    "model = RegressionModel(input_dim, output_dim)\n",
    "# train model\n",
    "train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f8fd71b98b163a0965b3204c263be7b56efe89ac907df8b2c30eb28f29cbfb8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
